{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RuthKassahun/Brain_Tissue_Segmentation/blob/main/PreProccessed_2D_unet_4versions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DKu0jOnLpdmS",
        "outputId": "5caf1323-824f-4851-e851-52eb3a9bd6e2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting simpleitk\n",
            "  Downloading SimpleITK-2.2.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (52.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.7/52.7 MB\u001b[0m \u001b[31m29.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: simpleitk\n",
            "Successfully installed simpleitk-2.2.1\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import nibabel as nib\n",
        "import matplotlib.pyplot as plt\n",
        "import glob\n",
        "import os\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "import cv2\n",
        "import glob\n",
        "import warnings\n",
        "import scipy.misc\n",
        "!pip install simpleitk\n",
        "import SimpleITK as sitk\n",
        "from scipy import ndimage\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.models import Model\n",
        "\n",
        "from keras.preprocessing.image import ImageDataGenerator"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import glob\n",
        "import warnings\n",
        "import scipy.misc\n",
        "import numpy as np\n",
        "import nibabel as nib\n",
        "!pip install simpleitk\n",
        "import SimpleITK as sitk\n",
        "from scipy import ndimage\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import tensorflow as tf\n",
        "#import tensorflow_addons as tfa\n",
        "from tensorflow import keras\n",
        "from keras.models import Model\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.layers import Conv2D, Dropout, MaxPooling2D, UpSampling2D, concatenate, Conv2DTranspose, Input"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o4d_0hoJDN5m",
        "outputId": "f501522e-5e04-4f0b-8675-6f34de85fe4d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: simpleitk in /usr/local/lib/python3.8/dist-packages (2.2.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PU_RRQswplFz",
        "outputId": "b473406e-8f15-4a28-e4f7-d5e1ed234fc4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow_addons"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FEw0FQgYSMTt",
        "outputId": "cffcc1d6-d88c-43e4-acdd-00d8e25348e1"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tensorflow_addons\n",
            "  Downloading tensorflow_addons-0.19.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m33.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.8/dist-packages (from tensorflow_addons) (2.7.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from tensorflow_addons) (21.3)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging->tensorflow_addons) (3.0.9)\n",
            "Installing collected packages: tensorflow_addons\n",
            "Successfully installed tensorflow_addons-0.19.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install keras_unet_collection"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yBhOH8eC9Lnz",
        "outputId": "4e0f7f9c-3bcd-4639-9048-91ab114318bb"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting keras_unet_collection\n",
            "  Downloading keras_unet_collection-0.1.13-py3-none-any.whl (67 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.9/67.9 KB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: keras_unet_collection\n",
            "Successfully installed keras_unet_collection-0.1.13\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Conv2D, Dropout, MaxPooling2D, UpSampling2D, concatenate, Conv2DTranspose, Input"
      ],
      "metadata": {
        "id": "e2qy01scBJrw"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_unet(img_size=PATCH_SIZE, n_classes=N_CLASSES, n_input_channels=N_INPUT_CHANNELS, scale=1):\n",
        "    inputs = keras.Input(shape=img_size + (n_input_channels, ))\n",
        "\n",
        "    # Encoding path\n",
        "    conv1 = layers.Conv2D(32*scale, (3, 3), padding=\"same\", activation='relu')(inputs)\n",
        "    max1 = layers.MaxPooling2D((2, 2))(conv1)\n",
        "\n",
        "    conv2 = layers.Conv2D(64*scale, (3, 3), padding=\"same\", activation='relu')(max1)\n",
        "    max2 = layers.MaxPooling2D((2, 2))(conv2)\n",
        "\n",
        "    conv3 = layers.Conv2D(128*scale, (3, 3), padding=\"same\", activation='relu')(max2)\n",
        "    max3 = layers.MaxPooling2D((2, 2))(conv3)\n",
        "\n",
        "    lat = layers.Conv2D(256*scale, (3, 3), padding=\"same\", activation='relu')(max3)\n",
        "\n",
        "    # Decoding path\n",
        "    up1 = layers.UpSampling2D((2, 2))(lat)\n",
        "    concat1 = layers.concatenate([conv3, up1], axis=-1)\n",
        "    conv4 = layers.Conv2D(128*scale, (3, 3), padding=\"same\", activation='relu')(concat1)\n",
        "    \n",
        "    up2 = layers.UpSampling2D((2, 2))(conv4)\n",
        "    concat2 = layers.concatenate([conv2, up2], axis=-1)\n",
        "    conv5 = layers.Conv2D(64*scale, (3, 3), padding=\"same\", activation='relu')(concat2)\n",
        "    \n",
        "    up3 = layers.UpSampling2D((2, 2))(conv5)\n",
        "    concat3 = layers.concatenate([conv1, up3], axis=-1)\n",
        "\n",
        "    conv6 = layers.Conv2D(32*scale, (3, 3), padding=\"same\", activation='relu')(concat3)\n",
        "\n",
        "    outputs = layers.Conv2D(n_classes, (1, 1), activation=\"softmax\")(conv6)\n",
        "\n",
        "    model = keras.Model(inputs, outputs)\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "HTAwQT0zAj4u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_unet2(img_size=PATCH_SIZE, n_classes=N_CLASSES, n_input_channels=N_INPUT_CHANNELS, scale=1):\n",
        "    inputs = keras.Input(shape=img_size + (n_input_channels, ))\n",
        "\n",
        "    # Encoding path\n",
        "    conv1 = layers.Conv2D(32*scale, (3, 3), padding=\"same\", activation='relu')(inputs)\n",
        "    max1 = layers.MaxPooling2D((2, 2))(conv1)\n",
        "\n",
        "    conv2 = layers.Conv2D(64*scale, (3, 3), padding=\"same\", activation='relu')(max1)\n",
        "    max2 = layers.MaxPooling2D((2, 2))(conv2)\n",
        "\n",
        "    conv3 = layers.Conv2D(128*scale, (3, 3), padding=\"same\", activation='relu')(max2)\n",
        "    max3 = layers.MaxPooling2D((2, 2))(conv3)\n",
        "\n",
        "    conv4 = layers.Conv2D(256*scale, (3, 3), padding=\"same\", activation='relu')(max3)\n",
        "    max4 = layers.MaxPooling2D((2, 2))(conv4)\n",
        "\n",
        "    lat = layers.Conv2D(512*scale, (3, 3), padding=\"same\", activation='relu')(max4)\n",
        "\n",
        "    # Decoding path\n",
        "    up1 = layers.UpSampling2D((2, 2))(lat)\n",
        "    concat1 = layers.concatenate([conv4, up1], axis=-1)\n",
        "    conv5 = layers.Conv2D(256*scale, (3, 3), padding=\"same\", activation='relu')(concat1)\n",
        "    \n",
        "    up2 = layers.UpSampling2D((2, 2))(conv5)\n",
        "    concat2 = layers.concatenate([conv3, up2], axis=-1)\n",
        "    conv6 = layers.Conv2D(128*scale, (3, 3), padding=\"same\", activation='relu')(concat2)\n",
        "    \n",
        "    up3 = layers.UpSampling2D((2, 2))(conv6)\n",
        "    concat3 = layers.concatenate([conv2, up3], axis=-1)\n",
        "    conv7 = layers.Conv2D(64*scale, (3, 3), padding=\"same\", activation='relu')(concat3)\n",
        "\n",
        "    up4 = layers.UpSampling2D((2, 2))(conv7)\n",
        "    concat4 = layers.concatenate([conv1, up4], axis=-1)\n",
        "    conv8 = layers.Conv2D(32*scale, (3, 3), padding=\"same\", activation='relu')(concat4)\n",
        "\n",
        "    outputs = layers.Conv2D(n_classes, (1, 1), activation=\"softmax\")(conv8)\n",
        "\n",
        "    model = keras.Model(inputs, outputs)\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "_yyhi9UIiMnS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_unet3(img_size=PATCH_SIZE, n_classes=N_CLASSES, n_input_channels=N_INPUT_CHANNELS, scale=1):\n",
        "    inputs = keras.Input(shape=img_size + (n_input_channels, ))\n",
        "\n",
        "    # Encoding path\n",
        "    conv1 = layers.Conv2D(32*scale, (3, 3), padding=\"same\", activation='relu')(inputs)\n",
        "    drop1 = layers.Dropout(rate=dropout_rate)(conv1, training=True)\n",
        "    max1 = layers.MaxPooling2D((2, 2))(drop1)\n",
        "\n",
        "    conv2 = layers.Conv2D(64*scale, (3, 3), padding=\"same\", activation='relu')(max1)\n",
        "    drop2 = layers.Dropout(rate=dropout_rate)(conv2, training=True)\n",
        "    max2 = layers.MaxPooling2D((2, 2))(drop2)\n",
        "\n",
        "    conv3 = layers.Conv2D(128*scale, (3, 3), padding=\"same\", activation='relu')(max2)\n",
        "    drop3 = layers.Dropout(rate=dropout_rate)(conv3, training=True)\n",
        "    max3 = layers.MaxPooling2D((2, 2))(drop3)\n",
        "\n",
        "    conv4 = layers.Conv2D(256*scale, (3, 3), padding=\"same\", activation='relu')(max3)\n",
        "    drop4 = layers.Dropout(rate=dropout_rate)(conv4, training=True)\n",
        "    max4 = layers.MaxPooling2D((2, 2))(drop4)\n",
        "\n",
        "    lat = layers.Conv2D(512*scale, (3, 3), padding=\"same\", activation='relu')(max4)\n",
        "    drop5 = layers.Dropout(rate=dropout_rate)(lat, training=True)\n",
        "\n",
        "    # Decoding path\n",
        "    up1 = layers.UpSampling2D((2, 2))(drop5)\n",
        "    concat1 = layers.concatenate([conv4, up1], axis=-1)\n",
        "    conv5 = layers.Conv2D(256*scale, (3, 3), padding=\"same\", activation='relu')(concat1)\n",
        "    drop6 = layers.Dropout(rate=dropout_rate)(conv5, training=True)\n",
        "    \n",
        "    up2 = layers.UpSampling2D((2, 2))(drop6)\n",
        "    concat2 = layers.concatenate([conv3, up2], axis=-1)\n",
        "    conv6 = layers.Conv2D(128*scale, (3, 3), padding=\"same\", activation='relu')(concat2)\n",
        "    drop7 = layers.Dropout(rate=dropout_rate)(conv6, training=True)\n",
        "    \n",
        "    up3 = layers.UpSampling2D((2, 2))(drop7)\n",
        "    concat3 = layers.concatenate([conv2, up3], axis=-1)\n",
        "    conv7 = layers.Conv2D(64*scale, (3, 3), padding=\"same\", activation='relu')(concat3)\n",
        "    drop8 = layers.Dropout(rate=dropout_rate)(conv7, training=True)\n",
        "\n",
        "    up4 = layers.UpSampling2D((2, 2))(drop8)\n",
        "    concat4 = layers.concatenate([conv1, up4], axis=-1)\n",
        "    conv8 = layers.Conv2D(32*scale, (3, 3), padding=\"same\", activation='relu')(concat4)\n",
        "    drop9 = layers.Dropout(rate=dropout_rate)(conv8, training=True)\n",
        "\n",
        "    outputs = layers.Conv2D(n_classes, (1, 1), activation=\"softmax\")(conv8)\n",
        "\n",
        "    model = keras.Model(inputs, outputs)\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "8gSP4c7tmWVG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_unet4(img_size=PATCH_SIZE, n_classes=N_CLASSES, n_input_channels=N_INPUT_CHANNELS, scale=1):\n",
        "    inputs = keras.Input(shape=img_size + (n_input_channels, ))\n",
        "\n",
        "    # Encoding path\n",
        "    conv1 = layers.BatchNormalization()(layers.Conv2D(32*scale, (3, 3), padding=\"same\", activation='relu')(inputs))\n",
        "    drop1 = layers.Dropout(rate=dropout_rate)(conv1, training=True)\n",
        "    max1 = layers.MaxPooling2D((2, 2))(drop1)\n",
        "\n",
        "    conv2 = layers.BatchNormalization()(layers.Conv2D(64*scale, (3, 3), padding=\"same\", activation='relu')(max1))\n",
        "    drop2 = layers.Dropout(rate=dropout_rate)(conv2, training=True)\n",
        "    max2 = layers.MaxPooling2D((2, 2))(drop2)\n",
        "\n",
        "    conv3 = layers.BatchNormalization()(layers.Conv2D(128*scale, (3, 3), padding=\"same\", activation='relu')(max2))\n",
        "    drop3 = layers.Dropout(rate=dropout_rate)(conv3, training=True)\n",
        "    max3 = layers.MaxPooling2D((2, 2))(drop3)\n",
        "\n",
        "    conv4 = layers.BatchNormalization()(layers.Conv2D(256*scale, (3, 3), padding=\"same\", activation='relu')(max3))\n",
        "    drop4 = layers.Dropout(rate=dropout_rate)(conv4, training=True)\n",
        "    max4 = layers.MaxPooling2D((2, 2))(drop4)\n",
        "\n",
        "    lat = layers.BatchNormalization()(layers.Conv2D(512*scale, (3, 3), padding=\"same\", activation='relu')(max4))\n",
        "    drop5 = layers.Dropout(rate=dropout_rate)(lat, training=True)\n",
        "\n",
        "    # Decoding path\n",
        "    up1 = layers.UpSampling2D((2, 2))(drop5)\n",
        "    concat1 = layers.concatenate([conv4, up1], axis=-1)\n",
        "    conv5 = layers.BatchNormalization()(layers.Conv2D(256*scale, (3, 3), padding=\"same\", activation='relu')(concat1))\n",
        "    drop6 = layers.Dropout(rate=dropout_rate)(conv5, training=True)\n",
        "    \n",
        "    up2 = layers.UpSampling2D((2, 2))(drop6)\n",
        "    concat2 = layers.concatenate([conv3, up2], axis=-1)\n",
        "    conv6 = layers.BatchNormalization()(layers.Conv2D(128*scale, (3, 3), padding=\"same\", activation='relu')(concat2))\n",
        "    drop7 = layers.Dropout(rate=dropout_rate)(conv6, training=True)\n",
        "    \n",
        "    up3 = layers.UpSampling2D((2, 2))(drop7)\n",
        "    concat3 = layers.concatenate([conv2, up3], axis=-1)\n",
        "    conv7 = layers.BatchNormalization()(layers.Conv2D(64*scale, (3, 3), padding=\"same\", activation='relu')(concat3))\n",
        "    drop8 = layers.Dropout(rate=dropout_rate)(conv7, training=True)\n",
        "\n",
        "    up4 = layers.UpSampling2D((2, 2))(drop8)\n",
        "    concat4 = layers.concatenate([conv1, up4], axis=-1)\n",
        "    conv8 = layers.BatchNormalization()(layers.Conv2D(32*scale, (3, 3), padding=\"same\", activation='relu')(concat4))\n",
        "    drop9 = layers.Dropout(rate=dropout_rate)(conv8, training=True)\n",
        "\n",
        "    outputs = layers.Conv2D(n_classes, (1, 1), activation=\"softmax\")(conv8)\n",
        "\n",
        "    model = keras.Model(inputs, outputs)\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "a1B6bxt8xnid"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install focal_loss"
      ],
      "metadata": {
        "id": "ImLqMhdEcuTU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from focal_loss import BinaryFocalLoss"
      ],
      "metadata": {
        "id": "fNBdOnBPcsoz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from focal_loss import sparse_categorical_focal_loss"
      ],
      "metadata": {
        "id": "wMl47Zb2dqbD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "eeYOrqTfpqRJ"
      },
      "outputs": [],
      "source": [
        "FNAME_PATTERN = '/content/drive/MyDrive/Processed_MISA_Final_Project/P_IBSR-{}-{}.nii'\n",
        "N_VOLUMES = 15\n",
        "IMAGE_SIZE = (256, 128, 256)\n",
        "\n",
        "# network parameters\n",
        "N_CLASSES = 4\n",
        "N_INPUT_CHANNELS = 1\n",
        "PATCH_SIZE = (32, 32)\n",
        "PATCH_STRIDE = (32, 32)\n",
        "\n",
        "# training, validation, test parameters\n",
        "TRAINING_VOLUMES = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
        "VALIDATION_VOLUMES = [10, 11, 12, 13, 14]\n",
        "#TEST_VOLUMES = [14]\n",
        "\n",
        "\n",
        "# data preparation parameters\n",
        "CONTENT_THRESHOLD = 0.3\n",
        "\n",
        "# training parameters\n",
        "N_EPOCHS = 500\n",
        "BATCH_SIZE = 32\n",
        "PATIENCE = 20\n",
        "MODEL_FNAME_PATTERN = 'model_v3.h5'\n",
        "# OPTIMISER = 'Adam'\n",
        "learning_rate=0.0001\n",
        "OPTIMISER = tf.keras.optimizers.legacy.Adam(learning_rate=learning_rate)\n",
        "# OPTIMISER = tf.keras.optimizers.legacy.SGD(learning_rate=learning_rate)\n",
        "LOSS = 'categorical_crossentropy'\n",
        "# LOSS ='sparse_categorical_crossentropy'\n",
        "# LOSS = tfa.losses.GIoULoss()\n",
        "#LOSS = BinaryFocalLoss(gamma=2)\n",
        "# LOSS = sparse_categorical_focal_loss(from_logits=False, class_weight=[0.1, 0.9, 1, 1])\n",
        "\n",
        "# LOSS = tf.keras.losses.SparseCategoricalCrossentropy()\n",
        "\n",
        "dropout_rate = 0.5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "SJnPts_nqol7"
      },
      "outputs": [],
      "source": [
        "def load_data(n_volumes=N_VOLUMES, image_size=IMAGE_SIZE, fname_pattern=FNAME_PATTERN) :\n",
        "  T1_volumes = np.zeros((n_volumes, *image_size, 1))\n",
        "  #T2_volumes = np.zeros((n_volumes, *image_size, 1))\n",
        "  labels = np.zeros((n_volumes, *image_size, 1))\n",
        "  for i in range(n_volumes) :\n",
        "    img_data = nib.load(fname_pattern.format(i+1, 'T1'))\n",
        "    img_dataRE = img_data.get_fdata()\n",
        "    T1_volumes[i] = img_dataRE.reshape((*image_size, 1))\n",
        "\n",
        "    # img_data = nib.load(fname_pattern.format(i+1, 'T2'))\n",
        "    # T2_volumes[i] = img_data.get_fdata()\n",
        "\n",
        "    seg_data = nib.load(fname_pattern.format(i+1, 'label'))\n",
        "    labels[i] = seg_data.get_fdata()\n",
        "\n",
        "  return (T1_volumes, labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "lPPthE1-qrPh"
      },
      "outputs": [],
      "source": [
        "(T1_volumes, labels) = load_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "eElM97nKrtO8"
      },
      "outputs": [],
      "source": [
        "training_volumes_T1 = T1_volumes[TRAINING_VOLUMES]\n",
        "#training_volumes_T2 = T2_volumes[TRAINING_VOLUMES]\n",
        "training_labels = labels[TRAINING_VOLUMES]\n",
        "\n",
        "validation_volumes_T1 = T1_volumes[VALIDATION_VOLUMES]\n",
        "#validation_volumes_T2 = T2_volumes[VALIDATION_VOLUMES]\n",
        "validation_labels = labels[VALIDATION_VOLUMES]\n",
        "\n",
        "#testing_volumes_T1 = T1_volumes[TEST_VOLUMES]\n",
        "#testing_volumes_T2 = T2_volumes[TEST_VOLUMES]\n",
        "#testing_labels = labels[TEST_VOLUMES]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "K6zNRpANrvVd"
      },
      "outputs": [],
      "source": [
        "def extract_patches(x, patch_size, patch_stride) :\n",
        "  return tf.image.extract_patches(\n",
        "    x,\n",
        "    sizes=[1, *patch_size, 1],\n",
        "    strides=[1, *patch_stride, 1],\n",
        "    rates=[1, 1, 1, 1],\n",
        "    padding='SAME', name=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "CDf7cEGxrxd8"
      },
      "outputs": [],
      "source": [
        "def extract_useful_patches(\n",
        "    volumes, labels,\n",
        "    image_size=IMAGE_SIZE,\n",
        "    patch_size=PATCH_SIZE,\n",
        "    stride=PATCH_STRIDE,\n",
        "    threshold=CONTENT_THRESHOLD,\n",
        "    num_classes=N_CLASSES) :\n",
        "  volumes = volumes.reshape([-1, image_size[1], image_size[2], 1])\n",
        "  labels = labels.reshape([-1, image_size[1], image_size[2], 1])\n",
        "\n",
        "\n",
        "  vol_patches = extract_patches(volumes, patch_size, stride).numpy()\n",
        "  seg_patches = extract_patches(labels, patch_size, stride).numpy()\n",
        "  \n",
        "  vol_patches = vol_patches.reshape([-1, *patch_size, 1])\n",
        "  seg_patches = seg_patches.reshape([-1, *patch_size, ])\n",
        "\n",
        "  foreground_mask = seg_patches != 0 \n",
        "\n",
        "  useful_patches = foreground_mask.sum(axis=(1, 2)) > threshold * np.prod(patch_size)\n",
        "\n",
        "  vol_patches = vol_patches[useful_patches]\n",
        "  seg_patches = seg_patches[useful_patches]\n",
        "  print(np.unique(seg_patches))\n",
        "  seg_patches = tf.keras.utils.to_categorical(seg_patches, num_classes=N_CLASSES, dtype='float32')\n",
        "  \n",
        "  print(np.unique(seg_patches))\n",
        "  return (vol_patches, seg_patches)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "hdk1hs_Zr0M3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1e05fc57-597a-49ad-ab5b-03b0039089fc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0. 1. 2. 3.]\n",
            "[0. 1.]\n",
            "[0. 1. 2. 3.]\n",
            "[0. 1.]\n"
          ]
        }
      ],
      "source": [
        "# extract patches from training set\n",
        "(training_patches_T1, training_patches_seg) = extract_useful_patches(training_volumes_T1, training_labels)\n",
        "#(training_patches_T2, _) = extract_useful_patches(training_volumes_T2, training_labels)\n",
        "\n",
        "# extract patches from validation set\n",
        "(validation_patches_T1, validation_patches_seg) = extract_useful_patches(validation_volumes_T1, validation_labels)\n",
        "#(validation_patches_T2, _) = extract_useful_patches(validation_volumes_T2, validation_labels)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train=np.dstack([training_patches_T1] * 3)\n",
        "X_train = X_train.reshape(-1, 32,32,3)\n",
        "# X_train = np.rollaxis(X_train, 3, 1)\n",
        "\n",
        "X_val=np.dstack([validation_patches_T1] * 3)\n",
        "X_val= X_val.reshape(-1, 32,32,3)\n",
        "# X_val = np.rollaxis(X_val, 3, 1)\n",
        "print(validation_patches_T1.shape)\n",
        "print(X_val.shape)\n",
        "\n",
        "print(training_patches_T1.shape)\n",
        "print(X_train.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9xu-inJpxRGS",
        "outputId": "ce5f98e2-b4a6-47c9-a6bd-e6071fbf5024"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(6648, 32, 32, 1)\n",
            "(6648, 32, 32, 3)\n",
            "(12747, 32, 32, 1)\n",
            "(12747, 32, 32, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "my_callbacks = [\n",
        "    tf.keras.callbacks.EarlyStopping(patience=PATIENCE), # early stopping\n",
        "    tf.keras.callbacks.ModelCheckpoint(filepath=MODEL_FNAME_PATTERN, save_best_only=True) # save the best based on validation\n",
        "]\n",
        "\n",
        "unet = get_unet4()\n",
        "unet.compile(optimizer=OPTIMISER, loss=LOSS, \n",
        "             metrics=tf.keras.metrics.MeanIoU(num_classes=4)\n",
        "            )\n",
        "unet.fit(\n",
        "    x=training_patches_T1,\n",
        "    # x=X_train, \n",
        "    y=training_patches_seg,\n",
        "    # validation_data=(X_val, validation_patches_seg),\n",
        "    validation_data=(validation_patches_T1, validation_patches_seg),\n",
        "    batch_size=BATCH_SIZE,\n",
        "    epochs=500,\n",
        "    callbacks=my_callbacks,\n",
        "    verbose=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z6g1Q2L3Ar1x",
        "outputId": "666910eb-f3ce-49f6-8744-e3781135904d"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/500\n",
            "399/399 [==============================] - 19s 22ms/step - loss: 0.4920 - mean_io_u: 0.3750 - val_loss: 0.5637 - val_mean_io_u: 0.3750\n",
            "Epoch 2/500\n",
            "399/399 [==============================] - 7s 18ms/step - loss: 0.3004 - mean_io_u: 0.3750 - val_loss: 0.2629 - val_mean_io_u: 0.3750\n",
            "Epoch 3/500\n",
            "399/399 [==============================] - 7s 18ms/step - loss: 0.2554 - mean_io_u: 0.3750 - val_loss: 0.2339 - val_mean_io_u: 0.3750\n",
            "Epoch 4/500\n",
            "399/399 [==============================] - 7s 18ms/step - loss: 0.2292 - mean_io_u: 0.3750 - val_loss: 0.2178 - val_mean_io_u: 0.3750\n",
            "Epoch 5/500\n",
            "399/399 [==============================] - 7s 18ms/step - loss: 0.2141 - mean_io_u: 0.3750 - val_loss: 0.2062 - val_mean_io_u: 0.3750\n",
            "Epoch 6/500\n",
            "399/399 [==============================] - 7s 18ms/step - loss: 0.2032 - mean_io_u: 0.3750 - val_loss: 0.1969 - val_mean_io_u: 0.3750\n",
            "Epoch 7/500\n",
            "399/399 [==============================] - 7s 18ms/step - loss: 0.1926 - mean_io_u: 0.3750 - val_loss: 0.1914 - val_mean_io_u: 0.3750\n",
            "Epoch 8/500\n",
            "399/399 [==============================] - 7s 18ms/step - loss: 0.1855 - mean_io_u: 0.3750 - val_loss: 0.1900 - val_mean_io_u: 0.3750\n",
            "Epoch 9/500\n",
            "399/399 [==============================] - 7s 18ms/step - loss: 0.1794 - mean_io_u: 0.3750 - val_loss: 0.1831 - val_mean_io_u: 0.3750\n",
            "Epoch 10/500\n",
            "399/399 [==============================] - 7s 18ms/step - loss: 0.1730 - mean_io_u: 0.3750 - val_loss: 0.1819 - val_mean_io_u: 0.3750\n",
            "Epoch 11/500\n",
            "399/399 [==============================] - 7s 18ms/step - loss: 0.1684 - mean_io_u: 0.3750 - val_loss: 0.1801 - val_mean_io_u: 0.3750\n",
            "Epoch 12/500\n",
            "399/399 [==============================] - 7s 18ms/step - loss: 0.1639 - mean_io_u: 0.3750 - val_loss: 0.1786 - val_mean_io_u: 0.3750\n",
            "Epoch 13/500\n",
            "399/399 [==============================] - 7s 18ms/step - loss: 0.1606 - mean_io_u: 0.3750 - val_loss: 0.1757 - val_mean_io_u: 0.3750\n",
            "Epoch 14/500\n",
            "399/399 [==============================] - 7s 18ms/step - loss: 0.1573 - mean_io_u: 0.3750 - val_loss: 0.1757 - val_mean_io_u: 0.3750\n",
            "Epoch 15/500\n",
            "399/399 [==============================] - 7s 18ms/step - loss: 0.1543 - mean_io_u: 0.3750 - val_loss: 0.1765 - val_mean_io_u: 0.3750\n",
            "Epoch 16/500\n",
            "399/399 [==============================] - 7s 18ms/step - loss: 0.1512 - mean_io_u: 0.3750 - val_loss: 0.1742 - val_mean_io_u: 0.3750\n",
            "Epoch 17/500\n",
            "399/399 [==============================] - 7s 18ms/step - loss: 0.1491 - mean_io_u: 0.3750 - val_loss: 0.1739 - val_mean_io_u: 0.3750\n",
            "Epoch 18/500\n",
            "399/399 [==============================] - 7s 19ms/step - loss: 0.1463 - mean_io_u: 0.3750 - val_loss: 0.1734 - val_mean_io_u: 0.3750\n",
            "Epoch 19/500\n",
            "399/399 [==============================] - 7s 18ms/step - loss: 0.1448 - mean_io_u: 0.3750 - val_loss: 0.1741 - val_mean_io_u: 0.3750\n",
            "Epoch 20/500\n",
            "399/399 [==============================] - 7s 18ms/step - loss: 0.1428 - mean_io_u: 0.3750 - val_loss: 0.1713 - val_mean_io_u: 0.3750\n",
            "Epoch 21/500\n",
            "399/399 [==============================] - 7s 18ms/step - loss: 0.1401 - mean_io_u: 0.3750 - val_loss: 0.1739 - val_mean_io_u: 0.3750\n",
            "Epoch 22/500\n",
            "399/399 [==============================] - 7s 18ms/step - loss: 0.1382 - mean_io_u: 0.3750 - val_loss: 0.1764 - val_mean_io_u: 0.3750\n",
            "Epoch 23/500\n",
            "399/399 [==============================] - 7s 18ms/step - loss: 0.1372 - mean_io_u: 0.3750 - val_loss: 0.1784 - val_mean_io_u: 0.3750\n",
            "Epoch 24/500\n",
            "399/399 [==============================] - 7s 18ms/step - loss: 0.1356 - mean_io_u: 0.3750 - val_loss: 0.1746 - val_mean_io_u: 0.3750\n",
            "Epoch 25/500\n",
            "399/399 [==============================] - 7s 18ms/step - loss: 0.1338 - mean_io_u: 0.3750 - val_loss: 0.1739 - val_mean_io_u: 0.3750\n",
            "Epoch 26/500\n",
            "399/399 [==============================] - 7s 19ms/step - loss: 0.1328 - mean_io_u: 0.3750 - val_loss: 0.1707 - val_mean_io_u: 0.3750\n",
            "Epoch 27/500\n",
            "399/399 [==============================] - 7s 18ms/step - loss: 0.1313 - mean_io_u: 0.3750 - val_loss: 0.1737 - val_mean_io_u: 0.3750\n",
            "Epoch 28/500\n",
            "399/399 [==============================] - 7s 18ms/step - loss: 0.1299 - mean_io_u: 0.3750 - val_loss: 0.1718 - val_mean_io_u: 0.3750\n",
            "Epoch 29/500\n",
            "399/399 [==============================] - 7s 18ms/step - loss: 0.1283 - mean_io_u: 0.3750 - val_loss: 0.1694 - val_mean_io_u: 0.3750\n",
            "Epoch 30/500\n",
            "399/399 [==============================] - 7s 18ms/step - loss: 0.1278 - mean_io_u: 0.3750 - val_loss: 0.1709 - val_mean_io_u: 0.3750\n",
            "Epoch 31/500\n",
            "399/399 [==============================] - 7s 18ms/step - loss: 0.1266 - mean_io_u: 0.3750 - val_loss: 0.1708 - val_mean_io_u: 0.3750\n",
            "Epoch 32/500\n",
            "399/399 [==============================] - 7s 18ms/step - loss: 0.1253 - mean_io_u: 0.3750 - val_loss: 0.1704 - val_mean_io_u: 0.3750\n",
            "Epoch 33/500\n",
            "399/399 [==============================] - 7s 18ms/step - loss: 0.1243 - mean_io_u: 0.3750 - val_loss: 0.1703 - val_mean_io_u: 0.3750\n",
            "Epoch 34/500\n",
            "399/399 [==============================] - 7s 18ms/step - loss: 0.1229 - mean_io_u: 0.3750 - val_loss: 0.1717 - val_mean_io_u: 0.3750\n",
            "Epoch 35/500\n",
            "399/399 [==============================] - 7s 18ms/step - loss: 0.1225 - mean_io_u: 0.3750 - val_loss: 0.1705 - val_mean_io_u: 0.3750\n",
            "Epoch 36/500\n",
            "399/399 [==============================] - 7s 18ms/step - loss: 0.1209 - mean_io_u: 0.3750 - val_loss: 0.1707 - val_mean_io_u: 0.3750\n",
            "Epoch 37/500\n",
            "399/399 [==============================] - 7s 18ms/step - loss: 0.1204 - mean_io_u: 0.3750 - val_loss: 0.1685 - val_mean_io_u: 0.3750\n",
            "Epoch 38/500\n",
            "399/399 [==============================] - 7s 18ms/step - loss: 0.1198 - mean_io_u: 0.3750 - val_loss: 0.1727 - val_mean_io_u: 0.3750\n",
            "Epoch 39/500\n",
            "399/399 [==============================] - 7s 18ms/step - loss: 0.1190 - mean_io_u: 0.3750 - val_loss: 0.1694 - val_mean_io_u: 0.3750\n",
            "Epoch 40/500\n",
            "399/399 [==============================] - 7s 18ms/step - loss: 0.1176 - mean_io_u: 0.3750 - val_loss: 0.1708 - val_mean_io_u: 0.3750\n",
            "Epoch 41/500\n",
            "399/399 [==============================] - 7s 18ms/step - loss: 0.1175 - mean_io_u: 0.3750 - val_loss: 0.1714 - val_mean_io_u: 0.3750\n",
            "Epoch 42/500\n",
            "399/399 [==============================] - 7s 18ms/step - loss: 0.1168 - mean_io_u: 0.3750 - val_loss: 0.1707 - val_mean_io_u: 0.3750\n",
            "Epoch 43/500\n",
            "399/399 [==============================] - 7s 18ms/step - loss: 0.1159 - mean_io_u: 0.3750 - val_loss: 0.1696 - val_mean_io_u: 0.3750\n",
            "Epoch 44/500\n",
            "399/399 [==============================] - 7s 18ms/step - loss: 0.1149 - mean_io_u: 0.3750 - val_loss: 0.1717 - val_mean_io_u: 0.3750\n",
            "Epoch 45/500\n",
            "399/399 [==============================] - 7s 18ms/step - loss: 0.1146 - mean_io_u: 0.3750 - val_loss: 0.1706 - val_mean_io_u: 0.3750\n",
            "Epoch 46/500\n",
            "399/399 [==============================] - 7s 18ms/step - loss: 0.1138 - mean_io_u: 0.3750 - val_loss: 0.1688 - val_mean_io_u: 0.3750\n",
            "Epoch 47/500\n",
            "399/399 [==============================] - 7s 18ms/step - loss: 0.1132 - mean_io_u: 0.3750 - val_loss: 0.1705 - val_mean_io_u: 0.3750\n",
            "Epoch 48/500\n",
            "399/399 [==============================] - 7s 18ms/step - loss: 0.1124 - mean_io_u: 0.3750 - val_loss: 0.1701 - val_mean_io_u: 0.3750\n",
            "Epoch 49/500\n",
            "399/399 [==============================] - 7s 18ms/step - loss: 0.1120 - mean_io_u: 0.3750 - val_loss: 0.1716 - val_mean_io_u: 0.3750\n",
            "Epoch 50/500\n",
            "399/399 [==============================] - 7s 18ms/step - loss: 0.1118 - mean_io_u: 0.3750 - val_loss: 0.1720 - val_mean_io_u: 0.3750\n",
            "Epoch 51/500\n",
            "399/399 [==============================] - 7s 18ms/step - loss: 0.1108 - mean_io_u: 0.3750 - val_loss: 0.1715 - val_mean_io_u: 0.3750\n",
            "Epoch 52/500\n",
            "399/399 [==============================] - 7s 18ms/step - loss: 0.1104 - mean_io_u: 0.3750 - val_loss: 0.1738 - val_mean_io_u: 0.3750\n",
            "Epoch 53/500\n",
            "399/399 [==============================] - 7s 18ms/step - loss: 0.1101 - mean_io_u: 0.3750 - val_loss: 0.1710 - val_mean_io_u: 0.3750\n",
            "Epoch 54/500\n",
            "399/399 [==============================] - 7s 18ms/step - loss: 0.1094 - mean_io_u: 0.3750 - val_loss: 0.1720 - val_mean_io_u: 0.3750\n",
            "Epoch 55/500\n",
            "399/399 [==============================] - 7s 18ms/step - loss: 0.1084 - mean_io_u: 0.3750 - val_loss: 0.1719 - val_mean_io_u: 0.3750\n",
            "Epoch 56/500\n",
            "399/399 [==============================] - 7s 18ms/step - loss: 0.1079 - mean_io_u: 0.3750 - val_loss: 0.1720 - val_mean_io_u: 0.3750\n",
            "Epoch 57/500\n",
            "399/399 [==============================] - 7s 18ms/step - loss: 0.1078 - mean_io_u: 0.3750 - val_loss: 0.1729 - val_mean_io_u: 0.3750\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f60a0f1d1f0>"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CQxLKUTNLUne"
      },
      "outputs": [],
      "source": [
        "# validation_volumes_T1_processed = validation_volumes_T1.reshape([-1, IMAGE_SIZE[1], IMAGE_SIZE[2], 1])\n",
        "# validation_labels_processed = validation_labels.reshape([-1, IMAGE_SIZE[1], IMAGE_SIZE[2]])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "validation_volumes_T1_processed = validation_volumes_T1.reshape([-1, PATCH_SIZE[0], PATCH_SIZE[1], 1])\n",
        "validation_labels_processed = validation_labels.reshape([-1, PATCH_SIZE[0], PATCH_SIZE[1]])\n",
        "validation_labels_processed=tf.keras.utils.to_categorical(validation_labels_processed, num_classes=4, dtype='float32')"
      ],
      "metadata": {
        "id": "AXV0b6EJrwHI"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "testing_volumes_T1 = validation_volumes_T1[1]\n",
        "testing_labels = validation_labels[1]\n",
        "testing_volumes_T1_processed = testing_volumes_T1.reshape([-1, PATCH_SIZE[0], PATCH_SIZE[1], 1])\n",
        "testing_labels_processed = testing_labels.reshape([-1, PATCH_SIZE[0], PATCH_SIZE[1]])\n",
        "# testing_labels_processed=tf.keras.utils.to_categorical(testing_labels_processed, num_classes=4, dtype='float32')"
      ],
      "metadata": {
        "id": "-qa4MnJE-R-2"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "testing_volumes_T1 = validation_volumes_T1[1]\n",
        "testing_labels = validation_labels[1]\n",
        "testing_volumes_T1_processed = testing_volumes_T1.reshape([-1, IMAGE_SIZE[1], IMAGE_SIZE[2], 1])\n",
        "testing_labels_processed = testing_labels.reshape([-1, IMAGE_SIZE[1], IMAGE_SIZE[2]])"
      ],
      "metadata": {
        "id": "r6acnrqpLI7G"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "unet = get_unet4(\n",
        "    img_size=(IMAGE_SIZE[1], IMAGE_SIZE[2]),\n",
        "    n_classes=N_CLASSES,\n",
        "    n_input_channels=N_INPUT_CHANNELS)\n",
        "unet.compile(optimizer=OPTIMISER, loss=LOSS)\n",
        "unet.load_weights(MODEL_FNAME_PATTERN)"
      ],
      "metadata": {
        "id": "r4P5iH4bMwfj"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "mh9LurKfsIG5"
      },
      "outputs": [],
      "source": [
        "def pred_val_data(x)  :\n",
        "  # creates probability map of each label for all volumes\n",
        "  #prediction = unet.predict(x=testing_volumes_processed)\n",
        "  prediction = unet.predict(x)\n",
        "  # print(np.unique(prediction))\n",
        "  # print(prediction.shape)\n",
        "\n",
        "  prediction = np.argmax(prediction, axis=3)\n",
        "\n",
        "  # plt.axis('off')\n",
        "  # plt.imshow(prediction[:, :, 150])\n",
        "\n",
        "  return prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "csfe_0Fr1l7J",
        "outputId": "b96c852f-6522-4581-e701-48717d48aeb3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8/8 [==============================] - 4s 70ms/step\n"
          ]
        }
      ],
      "source": [
        "prediction = pred_val_data(testing_volumes_T1_processed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8OfZVPXssOdI",
        "outputId": "8fb14e0c-6aef-4a08-aa0c-6d09d5910dff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting medpy\n",
            "  Downloading MedPy-0.4.0.tar.gz (151 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m151.8/151.8 KB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.8/dist-packages (from medpy) (1.7.3)\n",
            "Requirement already satisfied: numpy>=1.11.0 in /usr/local/lib/python3.8/dist-packages (from medpy) (1.21.6)\n",
            "Requirement already satisfied: SimpleITK>=1.1.0 in /usr/local/lib/python3.8/dist-packages (from medpy) (2.2.1)\n",
            "Building wheels for collected packages: medpy\n",
            "  Building wheel for medpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for medpy: filename=MedPy-0.4.0-cp38-cp38-linux_x86_64.whl size=753428 sha256=a42fc1c88f6b25a864471481c2889f87c4e06795dcd12db0b54cae33c86a8c35\n",
            "  Stored in directory: /root/.cache/pip/wheels/fc/38/7d/e0b8bcb2770f779a93cab5ab7fa6dd344011e1278cb90cab86\n",
            "Successfully built medpy\n",
            "Installing collected packages: medpy\n",
            "Successfully installed medpy-0.4.0\n"
          ]
        }
      ],
      "source": [
        "!pip install medpy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "nGRuOOSosQWX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "883699a6-36c8-4a70-eb03-50f9c354094e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dice coefficient class 0 equal to  1.00\n",
            "Dice coefficient class 1 equal to  0.88\n",
            "Dice coefficient class 2 equal to  0.92\n",
            "Dice coefficient class 3 equal to  0.91\n",
            "Hausdorff distance class 0 equal to  45.19\n",
            "Hausdorff distance class 1 equal to  114.59\n",
            "Hausdorff distance class 2 equal to  86.69\n",
            "Hausdorff distance class 3 equal to  69.38\n",
            "Dice coefficient class 0 avd -0.00\n",
            "Dice coefficient class 1 avd -0.02\n",
            "Dice coefficient class 2 avd  0.07\n",
            "Dice coefficient class 3 avd -0.09\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import nibabel as nib\n",
        "from medpy.metric.binary import dc, hd, ravd\n",
        "\n",
        "def compute_dice(prediction, reference) :\n",
        "  for c in np.unique(prediction) :\n",
        "    dsc_val = dc(prediction == c, reference==c)\n",
        "    print(f'Dice coefficient class {c} equal to {dsc_val : .2f}')\n",
        "\n",
        "def compute_hd(prediction, reference, voxel_spacing) :\n",
        "  for c in np.unique(prediction) :\n",
        "    hd_val = hd(prediction == c, reference==c, voxelspacing=voxel_spacing, connectivity=1)\n",
        "    print(f'Hausdorff distance class {c} equal to {hd_val : .2f}')\n",
        "\n",
        "def compute_ravd(prediction, reference) :\n",
        "  for c in np.unique(prediction) :\n",
        "    ravd_val = ravd(prediction == c, reference==c)\n",
        "    print(f'Dice coefficient class {c} avd {ravd_val : .2f}')\n",
        "\n",
        "compute_dice(prediction, testing_labels_processed)\n",
        "compute_hd(prediction, testing_labels_processed, [1, 1, 1])\n",
        "compute_ravd(prediction, testing_labels_processed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zls74XLDEMtQ",
        "outputId": "96d0c231-adce-450a-d323-74e01797e4de"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8/8 [==============================] - 1s 85ms/step\n",
            "Dice coefficient class 0 equal to  1.00\n",
            "Dice coefficient class 1 equal to  0.80\n",
            "Dice coefficient class 2 equal to  0.90\n",
            "Dice coefficient class 3 equal to  0.91\n",
            "Hausdorff distance class 0 equal to  40.12\n",
            "Hausdorff distance class 1 equal to  19.24\n",
            "Hausdorff distance class 2 equal to  83.67\n",
            "Hausdorff distance class 3 equal to  7.28\n",
            "Dice coefficient class 0 avd  0.00\n",
            "Dice coefficient class 1 avd  0.16\n",
            "Dice coefficient class 2 avd  0.03\n",
            "Dice coefficient class 3 avd -0.09\n",
            "8/8 [==============================] - 1s 84ms/step\n",
            "Dice coefficient class 0 equal to  1.00\n",
            "Dice coefficient class 1 equal to  0.89\n",
            "Dice coefficient class 2 equal to  0.92\n",
            "Dice coefficient class 3 equal to  0.91\n",
            "Hausdorff distance class 0 equal to  40.00\n",
            "Hausdorff distance class 1 equal to  110.61\n",
            "Hausdorff distance class 2 equal to  87.97\n",
            "Hausdorff distance class 3 equal to  7.35\n",
            "Dice coefficient class 0 avd -0.00\n",
            "Dice coefficient class 1 avd -0.00\n",
            "Dice coefficient class 2 avd  0.07\n",
            "Dice coefficient class 3 avd -0.09\n",
            "8/8 [==============================] - 1s 85ms/step\n",
            "Dice coefficient class 0 equal to  1.00\n",
            "Dice coefficient class 1 equal to  0.82\n",
            "Dice coefficient class 2 equal to  0.91\n",
            "Dice coefficient class 3 equal to  0.87\n",
            "Hausdorff distance class 0 equal to  39.00\n",
            "Hausdorff distance class 1 equal to  117.80\n",
            "Hausdorff distance class 2 equal to  81.67\n",
            "Hausdorff distance class 3 equal to  21.93\n",
            "Dice coefficient class 0 avd -0.00\n",
            "Dice coefficient class 1 avd  0.16\n",
            "Dice coefficient class 2 avd -0.06\n",
            "Dice coefficient class 3 avd  0.15\n",
            "8/8 [==============================] - 1s 82ms/step\n",
            "Dice coefficient class 0 equal to  1.00\n",
            "Dice coefficient class 1 equal to  0.89\n",
            "Dice coefficient class 2 equal to  0.93\n",
            "Dice coefficient class 3 equal to  0.93\n",
            "Hausdorff distance class 0 equal to  35.61\n",
            "Hausdorff distance class 1 equal to  40.42\n",
            "Hausdorff distance class 2 equal to  64.41\n",
            "Hausdorff distance class 3 equal to  9.90\n",
            "Dice coefficient class 0 avd -0.00\n",
            "Dice coefficient class 1 avd  0.01\n",
            "Dice coefficient class 2 avd  0.04\n",
            "Dice coefficient class 3 avd -0.00\n",
            "8/8 [==============================] - 1s 84ms/step\n",
            "Dice coefficient class 0 equal to  1.00\n",
            "Dice coefficient class 1 equal to  0.89\n",
            "Dice coefficient class 2 equal to  0.93\n",
            "Dice coefficient class 3 equal to  0.90\n",
            "Hausdorff distance class 0 equal to  40.51\n",
            "Hausdorff distance class 1 equal to  87.32\n",
            "Hausdorff distance class 2 equal to  72.89\n",
            "Hausdorff distance class 3 equal to  9.43\n",
            "Dice coefficient class 0 avd -0.00\n",
            "Dice coefficient class 1 avd  0.08\n",
            "Dice coefficient class 2 avd  0.04\n",
            "Dice coefficient class 3 avd -0.05\n",
            "Average Dice, HD, and RAVD for class 0 is 0.9968576343279851,39.04861022521313, and -0.00136612167951033\n",
            "Average Dice, HD, and RAVD for class 1 is 0.8568487970696035,75.0772014178172, and 0.08128776950333663\n",
            "Average Dice, HD, and RAVD for class 2 is 0.919342215562895,78.12214343381137, and 0.023730164552218765\n",
            "Average Dice, HD, and RAVD for class 3 is 0.9027383695977879,11.178753477151925, and -0.014646853679412028\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import nibabel as nib\n",
        "from medpy.metric.binary import dc, hd, ravd\n",
        "\n",
        "def compute_dice(prediction, reference) :\n",
        "  dice_scores = np.zeros((1,4))\n",
        "  for c in np.unique(prediction) :\n",
        "    dsc_val = dc(prediction == c, reference==c)\n",
        "    dice_scores[0,c] = dsc_val\n",
        "    print(f'Dice coefficient class {c} equal to {dsc_val : .2f}')\n",
        "  return dice_scores\n",
        "\n",
        "def compute_hd(prediction, reference, voxel_spacing) :\n",
        "  hd_scores = np.zeros((1,4))\n",
        "  for c in np.unique(prediction) :\n",
        "    hd_val = hd(prediction == c, reference==c, voxelspacing=voxel_spacing, connectivity=1)\n",
        "    hd_scores[0,c] = hd_val\n",
        "    print(f'Hausdorff distance class {c} equal to {hd_val : .2f}')\n",
        "  return hd_scores\n",
        "\n",
        "def compute_ravd(prediction, reference) :\n",
        "  ravd_scores = np.zeros((1,4))\n",
        "  for c in np.unique(prediction) :\n",
        "    ravd_val = ravd(prediction == c, reference==c)\n",
        "    ravd_scores[0,c] = ravd_val\n",
        "    print(f'Dice coefficient class {c} avd {ravd_val : .2f}')\n",
        "  return ravd_scores\n",
        "\n",
        "# compute_dice(prediction, testing_labels_processed)\n",
        "# compute_hd(prediction, testing_labels_processed, [1, 1, 1])\n",
        "# compute_ravd(prediction, testing_labels_processed)\n",
        "\n",
        "dice_list=np.zeros((len(validation_volumes_T1),4))\n",
        "hd_list=np.zeros((len(validation_volumes_T1),4))\n",
        "ravd_list=np.zeros((len(validation_volumes_T1),4))\n",
        "\n",
        "for i in range(0,len(validation_volumes_T1)):\n",
        "\n",
        "  testing_volumes_T1=validation_volumes_T1[i]\n",
        "  testing_labels=validation_labels[i]\n",
        "  testing_volumes_T1_processed = testing_volumes_T1.reshape([-1, IMAGE_SIZE[1], IMAGE_SIZE[2], 1])\n",
        "  testing_labels_processed = testing_labels.reshape([-1, IMAGE_SIZE[1], IMAGE_SIZE[2]])\n",
        "  prediction = pred_val_data(testing_volumes_T1_processed)\n",
        "\n",
        "  dice_list[i,:] = compute_dice(prediction, testing_labels_processed)\n",
        "  hd_list[i,:] = compute_hd(prediction, testing_labels_processed, [1, 1, 1])\n",
        "  ravd_list[i,:] = compute_ravd(prediction, testing_labels_processed)\n",
        "\n",
        "dice_total=dice_list.sum(axis=0)\n",
        "hd_total=hd_list.sum(axis=0)\n",
        "ravd_total=ravd_list.sum(axis=0)\n",
        "\n",
        "for i in range(0,4):\n",
        "  print(\"Average Dice, HD, and RAVD for class {} is {},{}, and {}\".format(i,dice_total[i]/len(validation_volumes_T1),hd_total[i]/len(validation_volumes_T1),ravd_total[i]/len(validation_volumes_T1)))"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mn-yTPP7v5bD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jsir_ZT9HfJL"
      },
      "outputs": [],
      "source": [
        "dice_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6e2ozfmWHsAb"
      },
      "outputs": [],
      "source": [
        "hd_list\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UIKZnqp4Htzw"
      },
      "outputs": [],
      "source": [
        "ravd_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pA0ZlghiHQTl"
      },
      "outputs": [],
      "source": [
        "for i in range(0,4):\n",
        "  print(\"Average Dice, HD, and RAVD for class {} is {},{}, and {}\".format(i,dice_total[i]/len(validation_volumes_T1),hd_total[i]/len(validation_volumes_T1),ravd_total[i]/len(validation_volumes_T1)))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}